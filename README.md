# papers

## BERT
1. Pre-training of Deep Bidirectional Transformers for Language Understanding \
https://arxiv.org/pdf/1810.04805
    - https://github.com/google-research/bert
    - Algorithm: WordPiece Tokenization
2. Attention Is All You Need\
https://arxiv.org/pdf/1706.03762

## XLM
1. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding (2021) \
    - https://github.com/microsoft/unilm/tree/master/layoutxlm 
