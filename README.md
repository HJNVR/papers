# papers

## BERT
codes: ./Bert/fine_tune_bert.ipynb

1. Pre-training of Deep Bidirectional Transformers for Language Understanding (2019) \
(https://arxiv.org/abs/1810.04805)
    - https://github.com/google-research/bert
    - Algorithm: 
        - WordPiece Tokenization
        - Byte-Pair Encoding      

2. Attention Is All You Need (2017)\
(https://arxiv.org/abs/1706.03762)
    - The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights

3. Question Answering on SQuAD with BERT \ 
(https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15792151.pdf)


## XLM
1. LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding (2021) \
    - https://github.com/microsoft/unilm/tree/master/layoutxlm 
